{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from server_pull import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Variables\n",
    "N = 1000000-4000      # the time (or round) - 1 million\n",
    "d = 24                   # number of possible variables\n",
    "Qt_a = 0\n",
    "Nt_a = 0      #number of times arm a has been selected prior to T\n",
    "                        #If Nt(a) = 0, then a is considered to be a maximizing action.\n",
    "c = 1                   \n",
    "sum_rewards = 0 \n",
    "\n",
    "hist_t = [] #holds the natural log of each round\n",
    "hist_achieved_rewards = [] #holds the history of the UCB CHOSEN cumulative rewards\n",
    "hist_arm_selected = []     #holds the history of the arm selected for each time\n",
    "hist_best_possible_rewards = [] #holds the history of OPTIMAL cumulative rewards\n",
    "hist_random_choice_rewards = [] #holds the history of RANDONMLY selected actions rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop through no of rounds #t = time\n",
    "for t in range(1,N+1):\n",
    "    \n",
    "    #update Values as of round t\n",
    "    Nt_a += 1\n",
    "    reward = pull('user7', 'XTkjktIc', 16)['Reward']\n",
    "    sum_rewards += reward\n",
    "    hist_achieved_rewards.append(reward)\n",
    "    hist_arm_selected.append(17)\n",
    "    \n",
    "    # We don't know other selections for time t. \n",
    "#     r_ = df.values[t,[1,2,3,4]]     #get all rewards for time t to a vector\n",
    "#     r_best = r_[np.argmax(r_)]      #select the best action\n",
    "    \n",
    "#     pick_random = random.randrange(d) #choose an action randomly\n",
    "#     r_random = r_[pick_random] #np.random.choice(r_) #select reward for random action\n",
    "#     if len(hist_achieved_rewards)>0:\n",
    "#         hist_achieved_rewards.append(hist_achieved_rewards[-1]+reward)\n",
    "# #         hist_best_possible_rewards.append(hist_best_possible_rewards[-1]+r_best)\n",
    "#         hist_random_choice_rewards.append(hist_random_choice_rewards[-1]+r_random)\n",
    "#     else:\n",
    "#         \n",
    "#         hist_best_possible_rewards.append(r_best)\n",
    "#         hist_random_choice_rewards.append(r_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Output the results(1000 pulls)\n",
    "pull_hist = pd.DataFrame({'rewards': hist_achieved_rewards, \n",
    "                          'arm_selected': hist_arm_selected})\n",
    "# results = pd.DataFrame({'Num_arms_selected': Nt_a, 'Sum_rewards_arms': sum_rewards})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pull_hist.to_csv('pull_hist.csv')\n",
    "# results.to_csv('pull_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
